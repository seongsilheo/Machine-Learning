{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) #require_grad는 학습할 것이라고 알려줌\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#loss(Mean Square Error)\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)  #parameter list로 만들어서 넣어줌\n",
    "\n",
    "optimizer.zero_grad()  #zero_grad로 gradient 초기화\n",
    "cost.backward() #gradient 계산 \n",
    "optimizer.step() #step()으로 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1867], requires_grad=True)\n",
      "tensor([0.0800], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypothersis가 더 나아졌는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2667],\n",
      "        [0.4533],\n",
      "        [0.6400]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.7710, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/1000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch 100/1000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch 200/1000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch 300/1000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch 400/1000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch 500/1000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch 600/1000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch 700/1000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch 800/1000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch 900/1000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "tensor([1.9709], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "W = torch.zeros(1, requires_grad=True) #require_grad는 학습할 것이라고 알려줌\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "np_epochs = 1000\n",
    "for epoch in range(np_epochs + 1):\n",
    "    \n",
    "    #H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 계선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:3d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, np_epochs, W.item(), b.item(), cost.item()))\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10, W: 0.000, Cost: 18.666666\n",
      "Epoch    1/10, W: 2.800, Cost: 60.479992\n",
      "Epoch    2/10, W: 1.680, Cost: 8.631467\n",
      "Epoch    3/10, W: 2.128, Cost: 23.751167\n",
      "Epoch    4/10, W: 1.949, Cost: 16.804136\n",
      "Epoch    5/10, W: 2.020, Cost: 19.439077\n",
      "Epoch    6/10, W: 1.992, Cost: 18.362087\n",
      "Epoch    7/10, W: 2.003, Cost: 18.789202\n",
      "Epoch    8/10, W: 1.999, Cost: 18.617767\n",
      "Epoch    9/10, W: 2.001, Cost: 18.686243\n",
      "Epoch   10/10, W: 2.000, Cost: 18.658838\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1)\n",
    "\n",
    "#learning rate 설정\n",
    "lr = 0.1\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = y_train * W\n",
    "    \n",
    "    # cost gradient 계산\n",
    "    cost = torch.mean((hypothesis - y_train) **2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    \n",
    "    print('Epoch {:4d}/{}, W: {:.3f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
    "    \n",
    "    #cost gradient로 H(x) 개선\n",
    "    W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다차원으로 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2038, 80.4776, 79.5864, 86.6683, 61.6065]) Cost: 9322.951172\n",
      "Epoch    2/20 hypothesis: tensor([104.8773, 125.5926, 124.2017, 135.2536,  96.1426]) Cost: 2931.251953\n",
      "Epoch    3/20 hypothesis: tensor([125.9965, 150.8837, 149.2125, 162.4900, 115.5034]) Cost: 922.593262\n",
      "Epoch    4/20 hypothesis: tensor([137.8355, 165.0618, 163.2332, 177.7584, 126.3570]) Cost: 291.350159\n",
      "Epoch    5/20 hypothesis: tensor([144.4721, 173.0100, 171.0930, 186.3176, 132.4417]) Cost: 92.974937\n",
      "Epoch    6/20 hypothesis: tensor([148.1923, 177.4658, 175.4991, 191.1158, 135.8529]) Cost: 30.632730\n",
      "Epoch    7/20 hypothesis: tensor([150.2776, 179.9637, 177.9690, 193.8056, 137.7653]) Cost: 11.040657\n",
      "Epoch    8/20 hypothesis: tensor([151.4464, 181.3642, 179.3536, 195.3134, 138.8376]) Cost: 4.883067\n",
      "Epoch    9/20 hypothesis: tensor([152.1014, 182.1494, 180.1297, 196.1586, 139.4389]) Cost: 2.947450\n",
      "Epoch   10/20 hypothesis: tensor([152.4684, 182.5897, 180.5647, 196.6324, 139.7762]) Cost: 2.338595\n",
      "Epoch   11/20 hypothesis: tensor([152.6740, 182.8367, 180.8085, 196.8980, 139.9655]) Cost: 2.146704\n",
      "Epoch   12/20 hypothesis: tensor([152.7890, 182.9752, 180.9452, 197.0468, 140.0718]) Cost: 2.085862\n",
      "Epoch   13/20 hypothesis: tensor([152.8533, 183.0531, 181.0217, 197.1302, 140.1316]) Cost: 2.066172\n",
      "Epoch   14/20 hypothesis: tensor([152.8891, 183.0968, 181.0645, 197.1769, 140.1653]) Cost: 2.059439\n",
      "Epoch   15/20 hypothesis: tensor([152.9090, 183.1214, 181.0885, 197.2030, 140.1843]) Cost: 2.056801\n",
      "Epoch   16/20 hypothesis: tensor([152.9199, 183.1353, 181.1019, 197.2176, 140.1952]) Cost: 2.055403\n",
      "Epoch   17/20 hypothesis: tensor([152.9258, 183.1433, 181.1093, 197.2258, 140.2015]) Cost: 2.054429\n",
      "Epoch   18/20 hypothesis: tensor([152.9290, 183.1479, 181.1134, 197.2303, 140.2053]) Cost: 2.053576\n",
      "Epoch   19/20 hypothesis: tensor([152.9305, 183.1506, 181.1157, 197.2328, 140.2076]) Cost: 2.052765\n",
      "Epoch   20/20 hypothesis: tensor([152.9312, 183.1522, 181.1169, 197.2341, 140.2090]) Cost: 2.051943\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75], \n",
    "                             [92, 88, 93], \n",
    "                             [89, 91, 90], \n",
    "                             [96, 98, 100], \n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "\n",
    "\n",
    "np_epochs = 20\n",
    "for epoch in range(np_epochs + 1):\n",
    "    #H(x) 계산\n",
    "    #hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 +b\n",
    "    hypothesis = x_train.matmul(W) + b \n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(epoch, np_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module: w와 b를 일일이 쓰는 것은 힘들므로 편리한 모듈 제공\n",
    "\n",
    "- nn.Module을 상속해서 모델 생성\n",
    "- nn.Linear(3,1)\n",
    "-> 입력 차원: 3, 출력차원: 1\n",
    "- Hypothesis 계산은 forward() 에서!\n",
    "- gradient 계산은 PyTorch가 알아서 해준다 backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "        def forward(Self, x):\n",
    "            return self.linear(x)\n",
    "        \n",
    "#hypothesis = model(x_train)\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# 모델 초기화\n",
    "#W = torch.zeros((3, 1), requires_grad = True)\n",
    "#b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#H(x) 계산\n",
    "#hypothesis = x_train.matmul(W) + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.mse_loss\n",
    "\n",
    "- torch.nn.functional에서 제공하는 loss function 사용\n",
    "- 쉽게 다른 loss와 교체 가능!(l1_loss, smooth_l1_loss 등...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-72400e388c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# cost 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# cost = torch.mean((hypothesis - y_train) ** 2)와 같음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# cost 계산\n",
    "#cost = F.mse_loss(prediction, y_train)   -> cost = torch.mean((hypothesis - y_train) ** 2)와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75], \n",
    "                             [92, 88, 93], \n",
    "                             [89, 91, 90], \n",
    "                             [96, 98, 100], \n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "#model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# optimizer 설정\n",
    "#optimizer = optim.SGD([W,b], lr=1e-5)\n",
    "\n",
    "np_epochs = 20\n",
    "#for epoch in range(np_epochs + 1):\n",
    "    #H(x) 계산\n",
    "    #hypothesis = x_train.matmul(W) + b \n",
    "    #hypothesis = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    #cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    #cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    #optimizer.zero_grad()\n",
    "    #cost.backward()\n",
    "    #optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
